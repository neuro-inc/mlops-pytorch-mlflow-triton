kind: live
title: mlops-pytorch-mlflow-triton

volumes:
  data:
    remote: storage:/$[[ project.owner ]]/$[[ flow.project_id ]]/data
    mount: /project/data
    local: data
  notebooks:
    remote: storage:/$[[ project.owner ]]/$[[ flow.project_id ]]/notebooks
    mount: /project/notebooks
    local: notebooks
  results:
    remote: storage:/$[[ project.owner ]]/$[[ flow.project_id ]]/results
    mount: /project/results
    local: results
  triton_model_repo:
    remote: storage:/$[[ project.owner ]]/$[[ flow.project_id ]]/triton_model_repo
    mount: /project/triton_model_repo

jobs:

  jupyter:
    image: ghcr.io/neuro-inc/base:v22.5.0-runtime
    preset: gpu-small
    http_port: 8888
    http_auth: True
    browse: True
    detach: True
    volumes:
      - $[[ volumes.data.ref_rw ]]
      - $[[ volumes.notebooks.ref_rw ]]
      - $[[ volumes.results.ref_rw ]]
      - ${{ volumes.triton_model_repo.ref_rw }}
    env:
      MLFLOW_TRACKING_URI: http://${{ inspect_job('mlflow_server').internal_hostname_named }}:5000
      TRITON_URL: http://${{ inspect_job('triton_server').internal_hostname_named }}:8000
      TRITON_MODEL_REPO: ${{ volumes.triton_model_repo.mount }}
    cmd: >-
      jupyter notebook
        --no-browser
        --ip=0.0.0.0
        --port=8888
        --allow-root
        --NotebookApp.token=
        --notebook-dir=$[[ volumes.notebooks.mount ]]
        --NotebookApp.shutdown_no_activity_timeout=900
        --MappingKernelManager.cull_idle_timeout=7200
        --MappingKernelManager.cull_connected=True

  mlflow_server:
    action: gh:neuro-actions/mlflow@v1.20.2
    args:
      backend_store_uri: sqlite:///${{ volumes.results.mount }}/mlflow.db
      default_artifact_root: ${{ volumes.results.mount }}
      volumes: "${{ to_json( [volumes.results.ref_rw] ) }}"

  triton_server:
    # note: the image version depends on the underlying drivers version.
    # compativility matrix is here: https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html
    # nvcr.io/nvidia/tritonserver:21.12-py3 is for Driver Version: 495.29.05, CUDA Version: 11.5
    image: nvcr.io/nvidia/tritonserver:21.12-py3
    preset: gpu-small
    detach: True
    volumes:
      - ${{ volumes.results.ref_ro }}
      - ${{ volumes.triton_model_repo.ref_rw }}
    http_port: 8000
    bash: |
      tritonserver \
        --model-repository=${{ volumes.triton_model_repo.mount }} \
        --model-control-mode=explicit \
        --strict-model-config=false
